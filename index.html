<!DOCTYPE html>
<html lang="ja">
<head>
  <meta charset="utf-8"/>
  <title>Seiya Ito</title>
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.2/css/bootstrap.min.css" integrity="sha384-Smlep5jCw/wG7hdkwQ/Z5nLIefveQRIY9nfy6xoR1uRYBtpZgI6339F5dgvm/e9B" crossorigin="anonymous">
</head>
<body>
  <div class="container">
    <br>
    <div class="row">
      <div class="col-md-4 col-lg-4">
        <div class="text-center">
          <img id="me" alt="" src="imgs/prof.jpg" width="200" />
        </div>
      </div>
      <div class="col-md-8 col-lg-8">
          <br>
          <h4><b>Seiya Ito&nbsp; (伊東 聖矢)</b></h4>
          <br>
          <p>
            I am an assistant professor at <a href="https://www.aoyama.ac.jp/">Aoyama Gakuin University</a>. My research focuses on computer vision and robotics.
          </p>
          <p>
            [<a href="mailto:ito.seiya.1127@gmail.com">Email</a>]
            [<a href="https://www.linkedin.com/in/seiyaito/">LinkedIn</a>]
          </p>
      </div>
    </div>
    <br>

    <h4>News</h4>
    <ul>
      <li>One papers has been accepted to <a href="https://www.accv2022.org/en/default.asp">ACCV 2022</a>.</li>
      <li>Two papers have been accepted to <a href="http://www.mva-org.jp/mva2021/">MVA 2021</a>.</li>
      <li>One paper has been accepted to MDPI Sensors [IF: 3.275]. "Latent 3D Volume for Joint Depth Estimation and Semantic Segmentation from a Single Image".</li>
      <li>One paper has been accepted to ACCV 2018 Workshop on Attention/Intention Understanding. "Human Action Recognition via Body Part Region Segmented Dense Trajectories".</li>
      <li>One paper has been accepted to ECCV 2018 Workshop on 3D Reconstruction in the Wild. "Deep Modular Network Architecture for Depth Estimation from Single Indoor Images".</li>
    </ul>

    <br>
    <h4>Education</h4>
    <table class="table">
      <tbody>
        <tr>
          <td width="20%">2018.4-2021.3</td>
          <td>
            Ph.D, Aoyama Gakuin University<br>
            Graduate School of Science and Engineering
          </td>
        </tr>
        <tr>
          <td>2016.4-2018.3</td>
          <td>
            M.S., Aoyama Gakuin University<br>
            Graduate School of Science and Engineering
          </td>
        </tr>
        <tr>
          <td>2012.4-2016.3</td>
          <td>
            B.E., Aoyama Gakuin University<br>
            Department of Integrated Information Technology
          </td>
        </tr>
      </tbody>
    </table>
    <br>

    <h4>Research Experience</h4>
    <table class="table">
      <tbody>
        <tr>
          <td width="20%">2021.4-2021.8</td>
          <td>
            Research Fellow of Japan Society for the Promotion of Science (PD)<br>
            Aoyama Gakuin University
          </td>
        </tr>
        <tr>
          <td width="20%">2020.4-2021.3</td>
          <td>
            Research Fellow of Japan Society for the Promotion of Science (DC2)<br>
            Aoyama Gakuin University
          </td>
        </tr>
        <tr>
          <td width="20%">2019.9-2021.3</td>
          <td>
            Research Intern, National Institute of Advanced Industrial Science and Technology (AIST)<br>
            Artificial Intelligence Research Center (AIRC)
          </td>
        </tr>
        <tr>
          <td width="20%">2018.11-2021.3</td>
          <td>
            Research Assistant, Aoyama Gakuin University<br>
            Center for Advanced Information technology Research (CAIR)
          </td>
        </tr>
        <tr>
          <td>2017.02-2018.3</td>
          <td>
            Research Intern, Nippon Telegraph and Telephone Corporation<br>
            NTT Communication Science Laboratories
          </td>
        </tr>
      </tbody>
    </table>

    <h4>Research Interests</h4>
    <ul>
      <li>3D Reconstruction</li>
      <li>Depth Estimation</li>
      <li>Semantic Segmentation</li>
    </ul>

    <h4>Journal Publications</h4>
    <ol>
      <li>
        <p><u>伊東 聖矢</u>, 金子 直史, 鷲見 和彦.<br>
          <b>自己教師あり学習を用いた多眼ステレオ</b><br>
          精密工学会誌, 2020 (in Japanese).<br>
          <i style="color: #999">(Self-Supervised Learning for Multi-View Stereo)</i><br>
          [<a href="https://www.jstage.jst.go.jp/article/jjspe/86/12/86_1042/_article/-char/en" target="blank">Paper</a>]
        </p>
      </li>

      <li>
        <p><u>Seiya Ito</u>, Naoshi Kaneko, Kazuhiko Sumi.<br>
          <b>Latent 3D Volume for Joint Depth Estimation and Semantic Segmentation from a Single Image</b><br>
          Sensors, 2020.<br>
          [<a href="https://doi.org/10.3390/s20205765" target="blank">Paper</a>]
        </p>
      </li>

      <li>
        <p>Junji Takahashi, Masato Kawabe, <u>Seiya Ito</u>, Naoshi Kaneko, Wataro Takahashi, Toshiki Sakamoto, Akihiro Shibata, Yong Yu.<br>
          <b>Image-retrieval Method Using Gradient Dilation Images for Cloud-based Positioning System with 3D Wireframe Map</b><br>
          Sensors and Materials, 2020.<br>
          [<a href="https://myukk.org/SM2017/article.php?ss=2678" target="blank">Paper</a>]
        </p>
      </li>

      <li>
        <p>釜田 祐哉, <u>伊東 聖矢</u>, 金子 直史, 鷲見 和彦.<br>
          <b>食品チラシ画像を用いたレシピ推薦システム</b><br>
          精密工学会誌, 2019 (in Japanese).<br>
          <i style="color: #999">(Automatic Recipe Recommendation from Food Flyers)</i><br>
          [<a href="https://www.jstage.jst.go.jp/article/jjspe/85/12/85_1127/_article/-char/en" target="blank">Paper</a>][<a href="https://aoyamacvl.github.io/recipe-from-flyers/" target="blank">Project Page</a>]
        </p>
      </li>

      <li>
        <p>
          Tomoya Kaneko, Junji Takahashi, <u>Seiya Ito</u>, Yoshito Tobe.<br>
          <b>A Hybrid Map with Permanent 3D Wireframes and Temporal Line Segments toward Long-term Visual Localization</b><br>
          SICE Journal of Control, Measurement, and System Integration (Special Issue on Fundamentals and Applications of Smart Sensing), 2019.<br>
          [<a href="https://www.jstage.jst.go.jp/article/jcmsi/12/4/12_149/_article/-char/en/" target="blank">Paper</a>]
        </p>
      </li>

      <li>
        <p>
          金子 直史, <u>伊東 聖矢</u>, 鷲見 和彦.<br>
          <b>ClothesAwarePoseNet: 衣服の領域分割を考慮した人物姿勢推定法</b><br>
          電子情報通信学会論文誌 D (画像の認識・理解特集号), 2018 (in Japanese).<br>
          <i style="color: #999">(ClothesAwarePoseNet: Two-Stream Convolutional Networks for Clothing-Aware Human Pose Estimation)</i><br>
          [<a href="https://search.ieice.org/bin/summary.php?id=j101-d_8_1130&category=D&year=2018&lang=J&abst=" target="blank">Paper</a>]
        </p>
      </li>

      <li>
        <p>
          <u>Seiya Ito</u>, Naoshi Kaneko, Takeshi Yoshida, and Kazuhiko Sumi.<br>
          <b>Detection of Defective Regions in 3D Reconstruction to Support Image Acquisition</b><br>
          SICE Journal of Control, Measurement, and System Integretion (Special Issue on Smart Sensing), 2017.<br>
          [<a href="https://www.jstage.jst.go.jp/article/jcmsi/10/5/10_402/_article/-char/en" target="blank">Paper</a>]
        </p>
      </li>
    </ol>
    <br>

    <h4>International Conference</h4>
    <ol>
      <li>
        <p>
          Kengo Murata, <u>Seiya Ito</u>, Kouzou Ohara.<br>
          <b> Learning and Transforming General Representations to Break Down Stability-Plasticity Dilemma</b><br>
          16th Asian Conference of Computer Vision (ACCV), 2022.<br>
          [<a href="#" target="blank">Paper</a>]
        </p>
      </li>
      <li>
        <p>
          Hiroki Kojima, Naoshi Kaneko, <u>Seiya Ito</u>, Kazuhiko Sumi.<br>
          <b>Multimodal Pseudo-Labeling Under Various Shooting Conditions: Case Study on RGB and IR Images</b><br>
          28th International Workshop on Frontiers of Computer Vision (IW-FCV), 2022.<br>
          [<a href="https://doi.org/10.1007/978-3-031-06381-7_9" target="blank">Paper</a>]
        </p>
      </li>
      <li>
        <p>
          <u>Seiya Ito</u>, Naoshi Kaneko, Kazuhiko Sumi.<br>
          <b>Dense Panoptic Mapping from RGB-D Sequences in Dynamic Scenes</b><br>
          28th International Workshop on Frontiers of Computer Vision (IW-FCV), 2022.<br>
          [<a href="#" target="blank">Paper</a>]
        </p>
      </li>
      <li>
        <p>
          <u>Seiya Ito</u>, Byeongjun Ju, Naoshi Kaneko, Kazuhiko Sumi.<br>
          <b>Viewpoint-independent Single-view 3D Object Reconstruction using Reinforcement Learning</b><br>
          17th International Conference on Computer Vision Theory and Applications (VISAPP), 2022.<br>
          [<a href="https://doi.org/10.5220/0010825900003124" target="blank">Paper</a>]
        </p>
      </li>
      <li>
        <p>
          <u>Seiya Ito</u>, Naoshi Kaneko, Kazuhiko Sumi.<br>
          <b>Seeing Farther Than Supervision: Self-supervised Depth Completion in Challenging Environments</b><br>
          17th International Conference on Machine Vision Applications (MVA), 2021.<br>
          [<a href="https://doi.org/10.23919/MVA51890.2021.9511354" target="blank">Paper</a>][<a href="https://github.com/seiyaito/sfts">Code</a>]
        </p>
      </li>
      <li>
        <p>
          Taku Fujitomi, <u>Seiya Ito</u>, Naoshi Kaneko, Kazuhiko Sumi.<br>
          <b>Bi-directional Recurrent MVSNet for High-resolution Multi-view Stereo</b><br>
          17th International Conference on Machine Vision Applications (MVA), 2021.<br>
          [<a href="https://doi.org/10.23919/MVA51890.2021.9511358" target="blank">Paper</a>]
        </p>
      </li>

      <li>
        <p>
          Takafumi Nagi, Naoshi Kaneko, <u>Seiya Ito</u>, Kazuhiko Sumi.<br>
          <b>Automatic Dataset Collection for Speech-Driven Gesture Generation</b><br>
          15th International Conference on Quality Control by Artificial Vision (QCAV), 2021.<br>
          [<a href="https://doi.org/10.1117/12.2591375" target="blank">Paper</a>]
        </p>
      </li>

      <li>
        <p>
          Gakuto Murayama, Naoshi Kaneko, <u>Seiya Ito</u>, Kazuhiko Sumi.<br>
          <b>Reducing Depth Ambiguity in 3D Human Pose and Body Shape Estimation</b><br>
          15th International Conference on Quality Control by Artificial Vision (QCAV), 2021.<br>
          [<a href="https://doi.org/10.1117/12.2591151" target="blank">Paper</a>]
        </p>
      </li>

      <li>
        <p>
          Hiroki Kojima, Naoshi Kaneko, <u>Seiya Ito</u>, Kazuhiko Sumi.<br>
          <b>You Don't Drink a Cupboard: Improving Egocentric Action Recognition with Co-Occurrence of Verbs and Nouns</b><br>
          15th International Conference on Quality Control by Artificial Vision (QCAV), 2021.<br>
          [<a href="https://doi.org/10.1117/12.2591298" target="blank">Paper</a>]
        </p>
      </li>

      <li>
        <p>
          Naoshi Kaneko, Mei Oyamay, Masaki Hayashiz, <u>Seiya Ito</u>, Kazuhiko Sumi.<br>
          <b>Feature Bridging Networks for 3D Human BodyShape Estimation from a Single Depth Map</b><br>
          4th International Conference on Imaging, Vision & Pattern Recognition (IVPR), 2020.<br>
          [<a href="https://doi.org/10.1109/ICIEVicIVPR48672.2020.9306656" target="blank">Paper</a>]
        </p>
      </li>

      <li>
        <p>
          Ryo Tamura, <u>Seiya Ito</u>, Naoshi Kaneko, Kazuhiko Sumi.<br>
          <b>Towards Detailed 3D Modeling: Mesh Super-Resolution via Deformation</b><br>
          4th International Conference on Imaging, Vision & Pattern Recognition (IVPR), 2020.<br>
          [<a href="https://doi.org/10.1109/ICIEVicIVPR48672.2020.9306533" target="blank">Paper</a>]
        </p>
      </li>

      <li>
        <p>
          Yusuke Nakasato, <u>Seiya Ito</u>, Naoshi Kaneko, Kazuhiko Sumi.<br>
          <b>Complex Nonlinear and Grid-Sampling Harmonic Convolution for Rotation Equivariant Image Recognition</b><br>
          International Workshop on Frontiers of Computer Vision (IW-FCV), 2020.<br>
          [<a href="#" target="blank">Paper</a>]
        </p>
      </li>

      <li>
        <p>
          Kazunari Takagi, <u>Seiya Ito</u>, Naoshi Kaneko, Kazuhiko Sumi.<br>
          <b>Boosting Monocular Depth Estimation with Channel Attention and Mutual Learning</b><br>
          3rd International Conference on Imaging, Vision & Pattern Recognition (IVPR), 2019.<br>
          [<a href="https://ieeexplore.ieee.org/document/8858565" target="blank">Paper</a>][<a href="https://github.com/kazutvoice05/TBDP-Net">Code</a>]
        </p>
      </li>

      <li>
        <p>
          Natsuki Hase, <u>Seiya Ito</u>, Naoshi Kaneko, Kazuhiko Sumi.<br>
          <b>Data Augmentation for Intra-Class Imbalance with Generative Adversarial Network</b><br>
          Quality Control by Artificial Vision (QCAV), 2019.<br>
          [<a href="https://www.spiedigitallibrary.org/conference-proceedings-of-spie/11172/1117206/Data-augmentation-for-intra-class-imbalance-with-generative-adversarial-network/10.1117/12.2521692.short?SSO=1" target="blank">Paper</a>]
        </p>
      </li>

      <li>
        <p>
          Nobuhiro Suga, <u>Seiya Ito</u>, Naoshi Kaneko, Kazuhiko Sumi.<br>
          <b>Multiple Human Tracking with Dual Cost Graphs</b><br>
          Quality Control by Artificial Vision (QCAV), 2019.<br>
          [<a href="https://www.spiedigitallibrary.org/conference-proceedings-of-spie/11172/111721A/Multiple-human-tracking-with-dual-cost-graphs/10.1117/12.2521701.short" target="blank">Paper</a>]
        </p>
      </li>

      <li>
        <p>
          Masato Fukuzaki, <u>Seiya Ito</u>, Naoshi Kaneko, Kazuhiko Sumi.<br>
          <b>Robot Grasp Planning with Integration Map of Graspability and Object Occupancy</b><br>
          International Workshop on Frontiers of Computer Vision (IW-FCV), 2019.<br>
          [<a href="#" target="blank">Paper</a>]
        </p>
      </li>

      <li>
        <p>
          Kaho Yamada, <u>Seiya Ito</u>, Naoshi Kaneko, Kazuhiko Sumi.<br>
          <b>Human Action Recognition via Body Part Region Segmented Dense Trajectories</b><br>
          ACCV 2018 Workshop on Attention/Intention Understanding, 2018.<br>
          [<a href="https://link.springer.com/chapter/10.1007/978-3-030-21074-8_6" target="blank">Paper</a>]
        </p>
      </li>

      <li>
        <p>
          <u>Seiya Ito</u>*, Naoshi Kaneko*, Yuma Shinohara, Kazuhiko Sumi (* equal contribution).<br>
          <b>Deep Modular Network Architecture for Depth Estimation from Single Indoor Images</b><br>
          ECCV 2018 Workshop on 3D Reconstruction in the Wild, 2018.<br>
          [<a href="https://openaccess.thecvf.com/content_ECCVW_2018/papers/11129/Ito_Deep_Modular_Network_Architecture_for_Depth_Estimation_from_Single_Indoor_ECCVW_2018_paper.pdf" target="blank">Paper</a>]
        </p>
      </li>

      <li>
        <p>
          <u>Seiya Ito</u>, Naoshi Kaneko, Junji Takahashi, and Kazuhiko Sumi.<br>
          <b>Global Localization from a Single Image in Known Indoor Environments</b><br>
          7th International Conference on Informatics, Electronics & Vision (ICIEV), 2018.<br>
          [<a href="https://ieeexplore.ieee.org/document/8641025" target="blank">Paper</a>]
        </p>
      </li>
    </ol>
    <br>

    <h4>Domestic Conference</h4>
    <ul><li>32 papers</li></ul>
    <br>

    <h4>Awards</h4>
    <ul>
      <li>2022 MIRU Interactive Presentation Award.</li>
      <li>2022 Interactive Session Audience Award in SSII 2022.</li>
      <li>2019 Best Paper Award in IVPR2019.</li>
      <li>2019 Best poster presentation award in <a href="https://mr.hanyang.ac.kr/IW-FCV2019/conference/#award">IW-FCV2019</a>. </li>
      <li>2018 Best Master Thesis Award, Graduate School of Science and Engineering, Intelligence and Information Course, Aoyama Gakuin University. (3 winners out of 29 students)</li>
      <li>2016 Student Encouragement Award of 78th IPSJ National Convention.</li>
      <li>2016 Best Graduation Thesis Award, Department of Integrated Information Technology, Aoyama Gakuin University. (7 winners out of 120 students)</li>
    </ul>
    <br>

    <h4>Grants</h4>
    <ul>
      <li>JSPS Research Fellowships for Young Scientists (DC2/PD) [Apr. 2020 - Mar. 2022]</li>
      <li>2019 Early Eagle Program, Aoyama Gakuin University-Supported Program (PI)</li>
      <li>2018 Early Eagle Program, Aoyama Gakuin University-Supported Program (PI)</li>
    </ul>
    <br>
  </div>
  <script crossorigin="anonymous" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" src="https://code.jquery.com/jquery-3.2.1.slim.min.js"></script>
  <script crossorigin="anonymous" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js"></script>
  <script crossorigin="anonymous" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js"></script>
</body>
</html>

